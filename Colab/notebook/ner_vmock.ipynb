{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\Prep\\repository\\NLP-with-ML\\nlpvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset('conllpp')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens      [EU, rejects, German, call, to, boycott, Briti...\n",
       "ner_tags                          [3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data['train'][:])[['tokens','ner_tags']].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = data['train'].features['ner_tags']\n",
    "\n",
    "index2tag = {index:tag for index, tag in enumerate(tags.feature.names)}\n",
    "tag2index = {tag:index for index, tag in enumerate(tags.feature.names)}\n",
    "index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = data['train'].features['ner_tags'].feature\n",
    "\n",
    "index2tag = {index:tag for index, tag in enumerate(tags.names)}\n",
    "tag2index = {tag:index for index, tag in enumerate(tags.names)}\n",
    "index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-ORG'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.int2str(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    tag_name = {'ner_tags_str' : [tags.int2str(index) for index in batch['ner_tags']]}\n",
    "    return tag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens          [EU, rejects, German, call, to, boycott, Briti...\n",
       "ner_tags                              [3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
       "ner_tags_str            [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data['train'][:])[['tokens','ner_tags', 'ner_tags_str']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = data['train'][0]['tokens']\n",
    "inputs = tokenizer(inputs, is_split_into_words= True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, words_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in words_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "\n",
    "            if label%2 == 1:\n",
    "                label = label +1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0] [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n"
     ]
    }
   ],
   "source": [
    "labels  = data['train'][0]['ner_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_labels_with_tokens(labels, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "  tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "  all_labels = examples['ner_tags']\n",
    "\n",
    "  new_labels = []\n",
    "  for i, labels in enumerate(all_labels):\n",
    "    word_ids = tokenized_inputs.word_ids(i)\n",
    "    new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "  tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "  return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, remove_columns=data['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
       "           119,   102],\n",
       "        [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from seqeval) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from seqeval) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: responses<0.19 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: packaging in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (1.24.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (2023.10.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (2.16.1)\n",
      "Requirement already satisfied: dill in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: multiprocess in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: xxhash in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: pandas in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: aiohttp in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: pyarrow-hotfix in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (15.0.0)\n",
      "Requirement already satisfied: filelock in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: colorama in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from pandas->evaluate) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval\n",
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "metric = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = data['train'].features['ner_tags']\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data['train'][0]['ner_tags']\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  logits, labels = eval_preds\n",
    "\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "  true_labels = [[label_names[l] for l in label if l!=-100] for label in labels]\n",
    "\n",
    "  true_predictions = [[label_names[p] for p,l in zip(prediction, label) if l!=-100]\n",
    "                      for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "  all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "  return {\"precision\": all_metrics['overall_precision'],\n",
    "          \"recall\": all_metrics['overall_recall'],\n",
    "          \"f1\": all_metrics['overall_f1'],\n",
    "          \"accuracy\": all_metrics['overall_accuracy']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i:label for i, label in enumerate(label_names)}\n",
    "label2id = {label:i for i, label in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                                                    model_checkpoint,\n",
    "                                                    id2label=id2label,\n",
    "                                                    label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\"distilbert-finetuned-ner\",\n",
    "                         evaluation_strategy = \"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         learning_rate = 2e-5,\n",
    "                         num_train_epochs=3,\n",
    "                         weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 500/5268 [09:39<1:32:47,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2829, 'learning_rate': 1.810174639331815e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1000/5268 [19:07<1:24:41,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1324, 'learning_rate': 1.6203492786636296e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1500/5268 [28:52<1:26:30,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0992, 'learning_rate': 1.4305239179954442e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 33%|███▎      | 1756/5268 [35:15<1:03:31,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09908612817525864, 'eval_precision': 0.8693982074263764, 'eval_recall': 0.9141703130259172, 'eval_f1': 0.8912223133716162, 'eval_accuracy': 0.972405368811444, 'eval_runtime': 89.1332, 'eval_samples_per_second': 36.462, 'eval_steps_per_second': 4.566, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 2000/5268 [40:00<1:03:16,  1.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0671, 'learning_rate': 1.240698557327259e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 2500/5268 [49:47<56:09,  1.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0616, 'learning_rate': 1.0508731966590738e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 3000/5268 [59:29<39:37,  1.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0493, 'learning_rate': 8.610478359908885e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 3500/5268 [1:09:08<33:25,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0502, 'learning_rate': 6.712224753227031e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 67%|██████▋   | 3512/5268 [1:10:53<31:21,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06899060308933258, 'eval_precision': 0.9098360655737705, 'eval_recall': 0.9340289464826658, 'eval_f1': 0.9217737917289486, 'eval_accuracy': 0.9817801848472362, 'eval_runtime': 91.2283, 'eval_samples_per_second': 35.625, 'eval_steps_per_second': 4.461, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 4000/5268 [1:20:27<23:35,  1.12s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0301, 'learning_rate': 4.8139711465451785e-06, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 4500/5268 [1:30:08<14:16,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0286, 'learning_rate': 2.9157175398633257e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 5000/5268 [1:39:46<05:39,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0293, 'learning_rate': 1.0174639331814731e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 5268/5268 [1:46:32<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07084232568740845, 'eval_precision': 0.9111183994752378, 'eval_recall': 0.9350387075058902, 'eval_f1': 0.9229235880398671, 'eval_accuracy': 0.98292812150468, 'eval_runtime': 93.7255, 'eval_samples_per_second': 34.676, 'eval_steps_per_second': 4.342, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5268/5268 [1:46:34<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6394.4526, 'train_samples_per_second': 6.587, 'train_steps_per_second': 0.824, 'train_loss': 0.08056521705360963, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.08056521705360963, metrics={'train_runtime': 6394.4526, 'train_samples_per_second': 6.587, 'train_steps_per_second': 0.824, 'train_loss': 0.08056521705360963, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset = tokenized_datasets['train'],\n",
    "                  eval_dataset = tokenized_datasets['validation'],\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"B:/Prep/repository/NLP-with-ML/Assignment/distilbert-finetuned-ner/checkpoint-5268\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "token_classifier(\"Can you google price of jim beam for me and I work in google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zip\n",
      "  Using cached zip-0.0.2.tar.gz (3.0 kB)\n",
      "Collecting Flask-Admin>=1.0.4\n",
      "  Using cached Flask_Admin-1.6.1-py3-none-any.whl (7.5 MB)\n",
      "Collecting Flask-Bootstrap>=2.2.2-1\n",
      "  Using cached Flask-Bootstrap-3.3.7.1.tar.gz (456 kB)\n",
      "Collecting Flask-Cache>=0.10.1\n",
      "  Using cached Flask-Cache-0.13.1.tar.gz (45 kB)\n",
      "Collecting Flask-FlatPages>=0.3\n",
      "  Using cached Flask_FlatPages-0.8.2-py3-none-any.whl (10 kB)\n",
      "Collecting Flask-Gravatar>=0.2.4\n",
      "  Using cached Flask_Gravatar-0.5.0-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting Flask-Login>=0.1.3\n",
      "  Using cached Flask_Login-0.6.3-py3-none-any.whl (17 kB)\n",
      "Collecting Flask-Mail>=0.7.4\n",
      "  Using cached Flask-Mail-0.9.1.tar.gz (45 kB)\n",
      "Collecting Flask-PyMongo>=0.2.1\n",
      "  Using cached Flask_PyMongo-2.3.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting Flask-Restless>=0.9.1\n",
      "  Using cached Flask-Restless-0.17.0.tar.gz (42 kB)\n",
      "Collecting Flask-SQLAlchemy>=0.16\n",
      "  Using cached flask_sqlalchemy-3.1.1-py3-none-any.whl (25 kB)\n",
      "Collecting Flask-Themes>=0.1.3\n",
      "  Using cached Flask-Themes-0.1.3.tar.gz (9.7 kB)\n",
      "Collecting Flask-Uploads>=0.1.3\n",
      "  Using cached Flask-Uploads-0.2.1.tar.gz (7.6 kB)\n",
      "Collecting Flask-WTF>=0.8.2\n",
      "  Using cached flask_wtf-1.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting Flask>=0.9\n",
      "  Using cached flask-3.0.2-py3-none-any.whl (101 kB)\n",
      "Collecting frozen-flask\n",
      "  Using cached frozen_flask-1.0.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: Jinja2>=2.6 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from zip) (3.1.3)\n",
      "Collecting Markdown>=2.2.1\n",
      "  Using cached Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: PyYAML>=3.11 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from zip) (6.0.1)\n",
      "Collecting SQLAlchemy>=0.8.0b2\n",
      "  Using cached SQLAlchemy-2.0.25-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Collecting Sphinx>=1.3.1\n",
      "  Using cached sphinx-7.1.2-py3-none-any.whl (3.2 MB)\n",
      "Collecting WTForms>=1.0.3\n",
      "  Using cached wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
      "Collecting Werkzeug>=0.8.3\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Collecting argparse>=1.2.1\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting blinker>=1.2\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting bumpversion>=0.5.3\n",
      "  Using cached bumpversion-0.6.0-py2.py3-none-any.whl (8.4 kB)\n",
      "Requirement already satisfied: click>=6.3 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from zip) (8.1.7)\n",
      "Requirement already satisfied: colorama>=0.3.7 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from zip) (0.4.6)\n",
      "Collecting coverage>=4.0\n",
      "  Using cached coverage-7.4.1-cp38-cp38-win_amd64.whl (209 kB)\n",
      "Collecting cryptography>=1.0.1\n",
      "  Using cached cryptography-42.0.2-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "Collecting flake8>=2.4.1\n",
      "  Using cached flake8-7.0.0-py2.py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: networkx>=1.11 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from zip) (3.1)\n",
      "Collecting pymongo>=2.5.1\n",
      "  Using cached pymongo-4.6.1-cp38-cp38-win_amd64.whl (472 kB)\n",
      "Collecting pytest>=2.8.3\n",
      "  Using cached pytest-8.0.0-py3-none-any.whl (334 kB)\n",
      "Requirement already satisfied: python-dateutil>=1.5 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from zip) (2.8.2)\n",
      "Requirement already satisfied: six>=1.10.0 in b:\\prep\\repository\\nlp-with-ml\\nlpvenv\\lib\\site-packages (from zip) (1.16.0)\n",
      "Collecting tox>=2.1.1\n",
      "  Using cached tox-4.12.1-py3-none-any.whl (154 kB)\n",
      "Collecting watchdog>=0.8.3\n",
      "  Using cached watchdog-4.0.0-py3-none-win_amd64.whl (82 kB)\n",
      "Collecting wheel>=0.23.0\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Collecting wsgiref>=0.1.2\n",
      "  Using cached wsgiref-0.1.2.zip (37 kB)\n",
      "Collecting zip\n",
      "  Using cached zip-0.0.1.tar.gz (2.3 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'b:\\Prep\\repository\\NLP-with-ML\\nlpvenv\\Scripts\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\whiz\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6o7zx6mv\\\\wsgiref_61c2bfd39c5446b2a52c62a18cb0493d\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\whiz\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6o7zx6mv\\\\wsgiref_61c2bfd39c5446b2a52c62a18cb0493d\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\whiz\\AppData\\Local\\Temp\\pip-pip-egg-info-l9253cx3'\n",
      "         cwd: C:\\Users\\whiz\\AppData\\Local\\Temp\\pip-install-6o7zx6mv\\wsgiref_61c2bfd39c5446b2a52c62a18cb0493d\\\n",
      "    Complete output (8 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\whiz\\AppData\\Local\\Temp\\pip-install-6o7zx6mv\\wsgiref_61c2bfd39c5446b2a52c62a18cb0493d\\setup.py\", line 5, in <module>\n",
      "        import ez_setup\n",
      "      File \"C:\\Users\\whiz\\AppData\\Local\\Temp\\pip-install-6o7zx6mv\\wsgiref_61c2bfd39c5446b2a52c62a18cb0493d\\ez_setup\\__init__.py\", line 170\n",
      "        print \"Setuptools version\",version,\"or greater has been installed.\"\n",
      "              ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"Setuptools version\",version,\"or greater has been installed.\")?\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/41/9e/309259ce8dff8c596e8c26df86dbc4e848b9249fd36797fd60be456f03fc/wsgiref-0.1.2.zip#sha256=c7e610c800957046c04c8014aab8cce8f0b9f0495c8cd349e57c1f7cabf40e79 (from https://pypi.org/simple/wsgiref/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'b:\\Prep\\repository\\NLP-with-ML\\nlpvenv\\Scripts\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\whiz\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6o7zx6mv\\\\zip_b6e40933b48c44a69434c21a3b0bc356\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\whiz\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6o7zx6mv\\\\zip_b6e40933b48c44a69434c21a3b0bc356\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\whiz\\AppData\\Local\\Temp\\pip-pip-egg-info-gi8411z_'\n",
      "         cwd: C:\\Users\\whiz\\AppData\\Local\\Temp\\pip-install-6o7zx6mv\\zip_b6e40933b48c44a69434c21a3b0bc356\\\n",
      "    Complete output (5 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\whiz\\AppData\\Local\\Temp\\pip-install-6o7zx6mv\\zip_b6e40933b48c44a69434c21a3b0bc356\\setup.py\", line 4, in <module>\n",
      "        from pip.req import parse_requirements\n",
      "    ModuleNotFoundError: No module named 'pip.req'\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/b1/e7/212bf1ce8d65b357c337da56934922d18c3845127e8a188182266157c663/zip-0.0.1.tar.gz#sha256=970ba619ab742b5023ff1dbe88ed82b150dacbbad28aacf4a5ec18c54639988e (from https://pypi.org/simple/zip/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "ERROR: Could not find a version that satisfies the requirement wsgiref>=0.1.2 (from zip) (from versions: 0.1, 0.1.1, 0.1.2)\n",
      "ERROR: No matching distribution found for wsgiref>=0.1.2\n",
      "WARNING: You are using pip version 21.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'b:\\Prep\\repository\\NLP-with-ML\\nlpvenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'zip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!zip -r \"distilbert_ner.zip\" \"B:/Prep/repository/NLP-with-ML/Assignment/distilbert-finetuned-ner/checkpoint-5268\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
